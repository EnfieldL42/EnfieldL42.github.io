<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN">
<html>
<head>
  <title>risks</title>
  	<link rel="stylesheet" href="../styles/mystyles.css">
  </head>

<body style="background-color:white;">
<font color="#000000">
<a href="https://ibb.co/NmrxPfB"><img src="https://i.ibb.co/xfYh0wT/coollogo-com-131453446.png" alt="coollogo-com-131453446" border="0"></a>

<!-- Site navigation menu -->

<ul class="navbar">
  <li><a href="../index.html">Home page</a>
  <li><a href="topic.html">Technology/Topic</a>
	<li><a href="opportunities.html">Opportunities</a>
	<li><a href="risks.html">Risks</a>
	<li><a href="choices.html">Choices</a>
	<li><a href="ethics.html">Ethical Reflections</a>
	<li><a href="references.html">References</a>
   <li><a href="process.html">Process Support</a>
</ul>

<!-- Main content -->
<h1>Technology Risks</h1>

<p>One of the most difficult aspects of securing machine learning systems is that data in these systems has a role in security from the outside. This makes securing your machine learning systems considerably more complex. In most cases, the data sets used to train a machine learning system account for 60% of the risk, while learning methods and source codes account for 40%. That is why it is critical for companies to focus all of their efforts on architectural risk assessments. According to research, performing an architectural risk analysis is a critical first step in protecting machine learning systems. There are over 70 dangers linked with machine learning systems, according to the paper. Another significant difficulty is safeguarding data that has become a vital element of a machine learning model. </p>
<p>Fooling the system </p>
<p>Giving harmful inputs to machine learning systems is one of the most popular ways to fool them into making erroneous predictions. Simply described, they are machine-based visual illusions that present individuals a visual that does not exist in the actual world and drive people to make decisions based on it. Because of its extensive coverage and attention, it poses a far greater hazard than other machine learning security threats. Machine learning models are generally the focus of this form of assault. </p>
<p>Data Poisoning </p>
<p>For learning purposes, machine learning systems rely on data. That is why it is critical for businesses to ensure the data's dependability, integrity, and security; otherwise, erroneous forecasts may result. Hackers are aware of this and attempt to steal data from machine learning systems. They tamper with, corrupt, and poison that data to the point where the entire machine learning system collapses. Businesses should pay careful attention to the situation and take steps to reduce the danger. Machine learning professionals should limit the amount of training data that cyber thieves can control and to what extent they can control it. Worse, you'll have to safeguard all data sources because attackers can modify any data source you use to train your machine learning systems. If you don't, the chances of your machine learning training going wrong skyrocket. </p>
<p>Manipulation of Online systems</p> 
<p>The majority of machine learning systems are connected to the internet, especially when in use and learning. This creates a window of opportunity for attackers to take advantage of. By providing incorrect system input, cyber thieves can push machine learning systems in the wrong direction, or even worse, steadily retrain them to execute on their commands and perform the wrong thing. Manipulation of an online machine learning system is not only simple, but it is also so undetectable that the victim will be unaware that their machine learning system is in the hands of a third party. Choosing the correct method, keeping track of data ownership, and simplifying and securing system operations are all things that machine learning experts may do to address this problem. </p>
<p>Transfer Learning attack</p>
<p>The majority of machine learning systems rely on a model that has previously been trained. By giving it particular training, that generic machine learning model is adjusted to fulfil specific needs. A transfer learning attack can be lethal at this point. If the model you chose is widely used, attackers may be able to use it to deceive your task-specific machine learning model. To detect these types of assaults, keep a look out for strange and unexpected machine learning activities. Because machine learning algorithms are utilised on purpose during transfers, the danger is increased, especially if the learning transfer is not for the intended purpose. Group posting models are preferable since they clearly explain what their systems perform and how they will manage risk. </p>
<p>Data Privacy and Confidentiality </p>
<p>Machine learning algorithms, as previously said, use data to train and learn. It's vital to protect the privacy and security of that data, especially when it's embedded in a machine learning model. Hackers can carry out data extraction attacks that go unnoticed, putting your entire machine learning system at danger. Even if those assaults fail, cyber thieves can use smaller sub symbolic function extraction attacks to carry out other forms of attacks, such as adversarial attacks with malicious inputs. This means you must protect your machine learning systems not only against data extraction attacks, but also from function extraction assaults.</p>

 </html>